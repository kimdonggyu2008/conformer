checkpoint: null
device: 'cuda'
gpu_id : 1

tokenizer:
  tokenizer_file: null
  vocab_path: 'files/vocab.txt'

data:
  sampling_rate: 16000
  n_mels: 80
  hop_length: 160
  n_ftt: 400
  win_length: ${data.n_ftt}
  spec_aug:
  freq_mask:
    max_freq: 27
  time_mask:
    ps: 0.05
    number_of_masks: 10
  training_file: "./LibriSpeech/manifest_bpe/tokenized_bpe_train_merged.tsv"
  testing_file: "./LibriSpeech/manifest_bpe/tokenized_bpe_test_merged.tsv"
  files_sep: '\t'
  csv_file_keys:
    duration: 'duration'
    path: 'path'
    text: 'text'

training:
  batch_size: 4
  epochs: 1
  checkpoints_dir: 'checkpoints_medium'
  optim:
    beta1: 0.9
    beta2: 0.98
    eps: 1e-9
    weight_decay: 1e-6
    warmup_staps: 1e4
    model_dim: ${model.model_dim}
    scaler: 0.05
    step_size: 1

model:
  model_dim: 256        # S: 144, M: 256, L: 512
  p_dropout: 0.1        # 보통 동일 (0.1)

  enc:
    enc_dim: ${model.model_dim}         # S: 144, M: 256, L: 512
    in_channels: ${data.n_mels}         # 보통 고정 (예: 80)
    kernel_size: 25                     # 보통 고정 (subsampling conv1d kernel)
    out_channels: 256                   # S: 128, M: 256, L: 256

    mhsa_params:
      enc_dim: ${model.enc.enc_dim}     # S: 144, M: 256, L: 512
      h: 4                              # S: 4, M: 4, L: 8
      p_dropout: ${model.p_dropout}     # 보통 동일 (0.1)
      device: ${device}

    conv_mod_params:
      enc_dim: ${model.enc.enc_dim}     # S: 144, M: 256, L: 512
      scaling_factor: 2                 # 보통 동일
      kernel_size: 31                   # S/M/L 모두 논문 기준 31~32
      p_dropout: ${model.p_dropout}

    feed_forward_params:
      enc_dim: ${model.enc.enc_dim}     # S: 144, M: 256, L: 512
      scaling_factor: 2                 # 보통 동일 (2)
      p_dropout: ${model.p_dropout}
      residual_scaler: 0.5              # 보통 동일

    num_blocks: 16                       # S: 16, M: 16, L: 17  ← 꼭 바꿔야 함
    p_dropout: ${model.p_dropout}

  dec:
    enc_dim: ${model.enc.enc_dim}       # S: 144, M: 256, L: 512
    dec_dim: 640
